Ques: ● Explain the need of Flume.
● Explain the working of Flume and its components in brief.

Ans:
1.)
# Need Of Flume:
- Assume an e-commerce web application wants to analyze the customer behavior from a particular region. To do so, they would need to move the available log data in to Hadoop for analysis. Here, Apache Flume comes to our rescue.

- Flume is used to move the log data generated by application servers into HDFS at a higher speed.

# Features of Flume
Some of the notable features of Flume are as follows −

1. Flume ingests log data from multiple web servers into a centralized store (HDFS, HBase) efficiently.

2. Using Flume, we can get the data from multiple servers immediately into Hadoop.

3. Along with the log files, Flume is also used to import huge volumes of event data produced by social networking sites like Facebook and Twitter, and e-commerce websites like Amazon and Flipkart.

4. Flume supports a large set of sources and destinations types.

5. Flume supports multi-hop flows, fan-in fan-out flows, contextual routing, etc.

6. Flume can be scaled horizontally.

2.)
# Working Of Flume:
*Apache Flume - Data Transfer In Hadoop:
- Big Data, as we know, is a collection of large datasets that cannot be processed using traditional computing techniques. Big Data, when analyzed, gives valuable results. Hadoop is an open-source framework that allows to store and process Big Data in a distributed environment across clusters of computers using simple programming models.

*Streaming / Log Data:
- Generally, most of the data that is to be analyzed will be produced by various data sources like applications servers, social networking sites, cloud servers, and enterprise servers. This data will be in the form of log files and events.

- Log file − In general, a log file is a file that lists events/actions that occur in an operating system. For example, web servers list every request made to the server in the log files.

On harvesting such log data, we can get information about −

1.the application performance and locate various software and hardware failures.
2.the user behavior and derive better business insights.
3.The traditional method of transferring data into the HDFS system is to use the put command. Let us see how to use the put command.

# Components:
Flume Agent contains three main components namely, source, channel, and sink.

# Source:
A source is the component of an Agent which receives data from the data generators and transfers it to one or more channels in the form of Flume events.

Apache Flume supports several types of sources and each source receives events from a specified data generator.

Example − Avro source, Thrift source, twitter 1% source etc.

# Channel:
A channel is a transient store which receives the events from the source and buffers them till they are consumed by sinks. It acts as a bridge between the sources and the sinks.

These channels are fully transactional and they can work with any number of sources and sinks.

Example − JDBC channel, File system channel, Memory channel, etc.

# Sink:
A sink stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels and delivers it to the destination. The destination of the sink might be another agent or the central stores.

Example − HDFS sink.
